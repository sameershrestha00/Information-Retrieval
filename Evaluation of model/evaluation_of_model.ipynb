{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a21928",
   "metadata": {},
   "source": [
    "# Simple Precision, Recall, and F-Measure Program\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This program demonstrates how to calculate the three fundamental evaluation metrics in Information Retrieval:\n",
    "\n",
    "1. **Precision**: How many retrieved documents are actually relevant?\n",
    "2. **Recall**: How many relevant documents were successfully retrieved?\n",
    "3. **F-measure**: The harmonic mean that balances precision and recall\n",
    "\n",
    "We'll use a simple example to show these calculations step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed2f32",
   "metadata": {},
   "source": [
    "## Mathematical Formulas\n",
    "\n",
    "### Precision\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} = \\frac{\\text{Relevant Retrieved}}{\\text{Total Retrieved}}$$\n",
    "\n",
    "### Recall  \n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} = \\frac{\\text{Relevant Retrieved}}{\\text{Total Relevant}}$$\n",
    "\n",
    "### F-Measure\n",
    "$$\\text{F-measure} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbc8e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Documents: ['doc1', 'doc2', 'doc3', 'doc4', 'doc5', 'doc6', 'doc7', 'doc8', 'doc9', 'doc10']\n",
      "Relevant Documents (Ground Truth): ['doc1', 'doc3', 'doc5', 'doc7', 'doc9']\n",
      "Retrieved Documents (System Output): ['doc1', 'doc2', 'doc3', 'doc4', 'doc6', 'doc7']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Documents in the collection\n",
    "all_documents = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\", \"doc6\", \"doc7\", \"doc8\", \"doc9\", \"doc10\"]\n",
    "\n",
    "# Ground truth: Which documents are actually relevant for our query\n",
    "relevant_documents = [\"doc1\", \"doc3\", \"doc5\", \"doc7\", \"doc9\"]  # 5 relevant documents\n",
    "\n",
    "# What our system retrieved\n",
    "retrieved_documents = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc6\", \"doc7\"]  # 6 retrieved documents\n",
    "\n",
    "print(\"All Documents:\", all_documents)\n",
    "print(\"Relevant Documents (Ground Truth):\", relevant_documents)\n",
    "print(\"Retrieved Documents (System Output):\", retrieved_documents)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8d5e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFUSION MATRIX ANALYSIS ===\n",
      "True Positives (TP):  ['doc7', 'doc3', 'doc1'] -> Count: 3\n",
      "False Positives (FP): ['doc6', 'doc2', 'doc4'] -> Count: 3\n",
      "False Negatives (FN): ['doc5', 'doc9'] -> Count: 2\n",
      "\n",
      "=== METRIC CALCULATIONS ===\n",
      "Precision = TP / (TP + FP) = 3 / (3 + 3) = 3/6 = 0.500\n",
      "Recall    = TP / (TP + FN) = 3 / (3 + 2) = 3/5 = 0.600\n",
      "F-measure = 2 * P * R / (P + R) = 2 * 0.500 * 0.600 / (0.500 + 0.600) = 0.545\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Precision: 0.500 (50.0%)\n",
      "Recall:    0.600 (60.0%)\n",
      "F-measure: 0.545 (54.5%)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate True Positives, False Positives, False Negatives\n",
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F-measure\n",
    "    \"\"\"\n",
    "    # Convert to sets for easy intersection operations\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "    \n",
    "    # True Positives: Documents that are both retrieved AND relevant\n",
    "    true_positives = retrieved_set.intersection(relevant_set)\n",
    "    tp_count = len(true_positives)\n",
    "    \n",
    "    # False Positives: Documents that are retrieved but NOT relevant  \n",
    "    false_positives = retrieved_set - relevant_set\n",
    "    fp_count = len(false_positives)\n",
    "    \n",
    "    # False Negatives: Documents that are relevant but NOT retrieved\n",
    "    false_negatives = relevant_set - retrieved_set\n",
    "    fn_count = len(false_negatives)\n",
    "    \n",
    "    print(\"=== CONFUSION MATRIX ANALYSIS ===\")\n",
    "    print(f\"True Positives (TP):  {list(true_positives)} -> Count: {tp_count}\")\n",
    "    print(f\"False Positives (FP): {list(false_positives)} -> Count: {fp_count}\")\n",
    "    print(f\"False Negatives (FN): {list(false_negatives)} -> Count: {fn_count}\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate Precision\n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    \n",
    "    # Calculate Recall\n",
    "    recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    \n",
    "    # Calculate F-measure\n",
    "    f_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_measure, tp_count, fp_count, fn_count\n",
    "\n",
    "# Calculate metrics for our example\n",
    "precision, recall, f_measure, tp, fp, fn = calculate_metrics(retrieved_documents, relevant_documents)\n",
    "\n",
    "print(\"=== METRIC CALCULATIONS ===\")\n",
    "print(f\"Precision = TP / (TP + FP) = {tp} / ({tp} + {fp}) = {tp}/{tp+fp} = {precision:.3f}\")\n",
    "print(f\"Recall    = TP / (TP + FN) = {tp} / ({tp} + {fn}) = {tp}/{tp+fn} = {recall:.3f}\")\n",
    "print(f\"F-measure = 2 * P * R / (P + R) = 2 * {precision:.3f} * {recall:.3f} / ({precision:.3f} + {recall:.3f}) = {f_measure:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"=== FINAL RESULTS ===\")\n",
    "print(f\"Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "print(f\"Recall:    {recall:.3f} ({recall*100:.1f}%)\")\n",
    "print(f\"F-measure: {f_measure:.3f} ({f_measure*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9d72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5726666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7a0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103980a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7929576c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
